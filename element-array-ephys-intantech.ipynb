{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8271cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datajoint as dj\n",
    "import pathlib\n",
    "import re\n",
    "import numpy as np\n",
    "import inspect\n",
    "import importlib\n",
    "from element_interface.utils import find_root_directory, find_full_path, dict_to_uuid\n",
    "\n",
    "from .readers import intantech, kilosort, openephys\n",
    "from . import probe\n",
    "\n",
    "schema = dj.schema()\n",
    "\n",
    "_linking_module = None\n",
    "\n",
    "\n",
    "def activate(ephys_schema_name, probe_schema_name=None, *, create_schema=True,\n",
    "             create_tables=True, linking_module=None):\n",
    "    \"\"\"\n",
    "    activate(ephys_schema_name, probe_schema_name=None, *, create_schema=True, create_tables=True, linking_module=None)\n",
    "        :param ephys_schema_name: schema name on the database server to activate the `ephys` element\n",
    "        :param probe_schema_name: schema name on the database server to activate the `probe` element\n",
    "         - may be omitted if the `probe` element is already activated\n",
    "        :param create_schema: when True (default), create schema in the database if it does not yet exist.\n",
    "        :param create_tables: when True (default), create tables in the database if they do not yet exist.\n",
    "        :param linking_module: a module name or a module containing the\n",
    "         required dependencies to activate the `ephys` element:\n",
    "            Upstream tables:\n",
    "                + Session: parent table to ProbeInsertion, typically identifying a recording session\n",
    "                + SkullReference: Reference table for InsertionLocation, specifying the skull reference\n",
    "                 used for probe insertion location (e.g. Bregma, Lambda)\n",
    "            Functions:\n",
    "                + get_ephys_root_data_dir() -> list\n",
    "                    Retrieve the root data directory - e.g. containing the raw ephys recording files for all subject/sessions.\n",
    "                    :return: a string for full path to the root data directory\n",
    "                + get_session_directory(session_key: dict) -> str\n",
    "                    Retrieve the session directory containing the recorded Neuropixels data for a given Session\n",
    "                    :param session_key: a dictionary of one Session `key`\n",
    "                    :return: a string for full path to the session directory\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(linking_module, str):\n",
    "        linking_module = importlib.import_module(linking_module)\n",
    "    assert inspect.ismodule(linking_module),\\\n",
    "        \"The argument 'dependency' must be a module's name or a module\"\n",
    "\n",
    "    global _linking_module\n",
    "    _linking_module = linking_module\n",
    "\n",
    "    probe.activate(probe_schema_name, create_schema=create_schema,\n",
    "                   create_tables=create_tables)\n",
    "    schema.activate(ephys_schema_name, create_schema=create_schema,\n",
    "                    create_tables=create_tables, add_objects=_linking_module.__dict__)\n",
    "\n",
    "\n",
    "# -------------- Functions required by the elements-ephys  ---------------\n",
    "\n",
    "def get_ephys_root_data_dir() -> list:\n",
    "    \"\"\"\n",
    "    All data paths, directories in DataJoint Elements are recommended to be \n",
    "    stored as relative paths, with respect to some user-configured \"root\" \n",
    "    directory, which varies from machine to machine (e.g. different mounted \n",
    "    drive locations)\n",
    "    get_ephys_root_data_dir() -> list\n",
    "        This user-provided function retrieves the possible root data directories\n",
    "         containing the ephys data for all subjects/sessions\n",
    "         (e.g. acquired intantech or Open Ephys raw files,\n",
    "         output files from spike sorting routines, etc.)\n",
    "        :return: a string for full path to the ephys root data directory,\n",
    "         or list of strings for possible root data directories\n",
    "    \"\"\"\n",
    "    return _linking_module.get_ephys_root_data_dir()\n",
    "\n",
    "\n",
    "def get_session_directory(session_key: dict) -> str:\n",
    "    \"\"\"\n",
    "    get_session_directory(session_key: dict) -> str\n",
    "        Retrieve the session directory containing the\n",
    "         recorded Neuropixels data for a given Session\n",
    "        :param session_key: a dictionary of one Session `key`\n",
    "        :return: a string for relative or full path to the session directory\n",
    "    \"\"\"\n",
    "    return _linking_module.get_session_directory(session_key)\n",
    "\n",
    "\n",
    "# ----------------------------- Table declarations ----------------------\n",
    "\n",
    "\n",
    "@schema\n",
    "class AcquisitionSoftware(dj.Lookup):\n",
    "    definition = \"\"\"  # Name of software used for recording of neuropixels probes - intantech or Open Ephys\n",
    "    acq_software: varchar(24)    \n",
    "    \"\"\"\n",
    "    contents = zip(['intantech', 'Open Ephys'])\n",
    "\n",
    "\n",
    "@schema\n",
    "class ProbeInsertion(dj.Manual):\n",
    "    definition = \"\"\"\n",
    "    # Probe insertion implanted into an animal for a given session.\n",
    "    -> Session\n",
    "    insertion_number: tinyint unsigned\n",
    "    ---\n",
    "    -> probe.Probe\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "@schema\n",
    "class InsertionLocation(dj.Manual):\n",
    "    definition = \"\"\"\n",
    "    # Brain Location of a given probe insertion.\n",
    "    -> ProbeInsertion\n",
    "    ---\n",
    "    -> SkullReference\n",
    "    ap_location: decimal(6, 2) # (um) anterior-posterior; ref is 0; more anterior is more positive\n",
    "    ml_location: decimal(6, 2) # (um) medial axis; ref is 0 ; more right is more positive\n",
    "    depth:       decimal(6, 2) # (um) manipulator depth relative to surface of the brain (0); more ventral is more negative\n",
    "    theta=null:  decimal(5, 2) # (deg) - elevation - rotation about the ml-axis [0, 180] - w.r.t the z+ axis\n",
    "    phi=null:    decimal(5, 2) # (deg) - azimuth - rotation about the dv-axis [0, 360] - w.r.t the x+ axis\n",
    "    beta=null:   decimal(5, 2) # (deg) rotation about the shank of the probe [-180, 180] - clockwise is increasing in degree - 0 is the probe-front facing anterior\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "@schema\n",
    "class EphysRecording(dj.Imported):\n",
    "    definition = \"\"\"\n",
    "    # Ephys recording from a probe insertion for a given session.\n",
    "    -> ProbeInsertion      \n",
    "    ---\n",
    "    -> probe.ElectrodeConfig\n",
    "    -> AcquisitionSoftware\n",
    "    sampling_rate: float # (Hz) \n",
    "    \"\"\"\n",
    "\n",
    "    class EphysFile(dj.Part):\n",
    "        definition = \"\"\"\n",
    "        # Paths of files of a given EphysRecording round.\n",
    "        -> master\n",
    "        file_path: varchar(255)  # filepath relative to root data directory\n",
    "        \"\"\"\n",
    "\n",
    "    def make(self, key):\n",
    "\n",
    "        session_dir = find_full_path(get_ephys_root_data_dir(), \n",
    "                                     get_session_directory(key))\n",
    "\n",
    "        inserted_probe_serial_number = (ProbeInsertion * probe.Probe & key).fetch1('probe')\n",
    "\n",
    "        # search session dir and determine acquisition software\n",
    "        for ephys_pattern, ephys_acq_type in zip(['*.ap.meta', '*.oebin'],\n",
    "                                                 ['intantech', 'Open Ephys']):\n",
    "            ephys_meta_filepaths = [fp for fp in session_dir.rglob(ephys_pattern)]\n",
    "            if ephys_meta_filepaths:\n",
    "                acq_software = ephys_acq_type\n",
    "                break\n",
    "        else:\n",
    "            raise FileNotFoundError(\n",
    "                f'Ephys recording data not found!'\n",
    "                f' Neither intantech nor Open Ephys recording files found'\n",
    "                f' in {session_dir}')\n",
    "\n",
    "        if acq_software == 'intantech':\n",
    "            for meta_filepath in ephys_meta_filepaths:\n",
    "                intantech_meta = intantech.IntanTECHMeta(meta_filepath)\n",
    "                if str(intantech_meta.probe_SN) == inserted_probe_serial_number:\n",
    "                    break\n",
    "            else:\n",
    "                raise FileNotFoundError(\n",
    "                    'No intantech data found for probe insertion: {}'.format(key))\n",
    "\n",
    "            if re.search('(1.0|2.0)', intantech_meta.probe_model):\n",
    "                probe_type = intantech_meta.probe_model\n",
    "                electrode_query = probe.ProbeType.Electrode & {'probe_type': probe_type}\n",
    "\n",
    "                probe_electrodes = {\n",
    "                    (shank, shank_col, shank_row): key\n",
    "                    for key, shank, shank_col, shank_row in zip(*electrode_query.fetch(\n",
    "                        'KEY', 'shank', 'shank_col', 'shank_row'))}\n",
    "\n",
    "                electrode_group_members = [\n",
    "                    probe_electrodes[(shank, shank_col, shank_row)]\n",
    "                    for shank, shank_col, shank_row, _ in intantech_meta.shankmap['data']]\n",
    "            else:\n",
    "                raise NotImplementedError(\n",
    "                    'Processing for neuropixels probe model'\n",
    "                    ' {} not yet implemented'.format(intantech_meta.probe_model))\n",
    "\n",
    "            self.insert1({**key,\n",
    "                          **generate_electrode_config(probe_type, electrode_group_members),\n",
    "                          'acq_software': acq_software,\n",
    "                          'sampling_rate': intantech_meta.meta['imSampRate']})\n",
    "\n",
    "            root_dir = find_root_directory(get_ephys_root_data_dir(), \n",
    "                                           meta_filepath)\n",
    "            self.EphysFile.insert1({\n",
    "                **key,\n",
    "                'file_path': meta_filepath.relative_to(root_dir).as_posix()})\n",
    "        elif acq_software == 'Open Ephys':\n",
    "            dataset = openephys.OpenEphys(session_dir)\n",
    "            for serial_number, probe_data in dataset.probes.items():\n",
    "                if str(serial_number) == inserted_probe_serial_number:\n",
    "                    break\n",
    "            else:\n",
    "                raise FileNotFoundError(\n",
    "                    'No Open Ephys data found for probe insertion: {}'.format(key))\n",
    "\n",
    "            if re.search('(1.0|2.0)', probe_data.probe_model):\n",
    "                probe_type = probe_data.probe_model\n",
    "                electrode_query = probe.ProbeType.Electrode & {'probe_type': probe_type}\n",
    "\n",
    "                probe_electrodes = {key['electrode']: key\n",
    "                                    for key in electrode_query.fetch('KEY')}\n",
    "\n",
    "                electrode_group_members = [\n",
    "                    probe_electrodes[channel_idx]\n",
    "                    for channel_idx in probe_data.ap_meta['channels_ids']]\n",
    "            else:\n",
    "                raise NotImplementedError(\n",
    "                    'Processing for neuropixels'\n",
    "                    ' probe model {} not yet implemented'.format(probe_data.probe_model))\n",
    "\n",
    "            self.insert1({**key,\n",
    "                          **generate_electrode_config(probe_type, electrode_group_members),\n",
    "                          'acq_software': acq_software,\n",
    "                          'sampling_rate': probe_data.ap_meta['sample_rate']})\n",
    "\n",
    "            root_dir = find_root_directory(get_ephys_root_data_dir(),\n",
    "                probe_data.recording_info['recording_files'][0])\n",
    "            self.EphysFile.insert([{**key,\n",
    "                                    'file_path': fp.relative_to(root_dir).as_posix()}\n",
    "                                   for fp in probe_data.recording_info['recording_files']])\n",
    "        else:\n",
    "            raise NotImplementedError(f'Processing ephys files from'\n",
    "                                      f' acquisition software of type {acq_software} is'\n",
    "                                      f' not yet implemented')\n",
    "\n",
    "\n",
    "@schema\n",
    "class LFP(dj.Imported):\n",
    "    definition = \"\"\"\n",
    "    # Acquired local field potential (LFP) from a given Ephys recording.\n",
    "    -> EphysRecording\n",
    "    ---\n",
    "    lfp_sampling_rate: float   # (Hz)\n",
    "    lfp_time_stamps: longblob  # (s) timestamps with respect to the start of the recording (recording_timestamp)\n",
    "    lfp_mean: longblob         # (uV) mean of LFP across electrodes - shape (time,)\n",
    "    \"\"\"\n",
    "\n",
    "    class Electrode(dj.Part):\n",
    "        definition = \"\"\"\n",
    "        -> master\n",
    "        -> probe.ElectrodeConfig.Electrode  \n",
    "        ---\n",
    "        lfp: longblob               # (uV) recorded lfp at this electrode \n",
    "        \"\"\"\n",
    "\n",
    "    # Only store LFP for every 9th channel, due to high channel density,\n",
    "    # close-by channels exhibit highly similar LFP\n",
    "    _skip_channel_counts = 9\n",
    "\n",
    "    def make(self, key):\n",
    "        acq_software, probe_sn = (EphysRecording\n",
    "                                  * ProbeInsertion & key).fetch1('acq_software', 'probe')\n",
    "\n",
    "        electrode_keys, lfp = [], []\n",
    "\n",
    "        if acq_software == 'IntanTECH':\n",
    "            intantech_meta_filepath = get_intantech_meta_filepath(key)\n",
    "            intantech_recording = intantech.IntanTECH(intantech_meta_filepath.parent)\n",
    "\n",
    "            lfp_channel_ind = intantech_recording.lfmeta.recording_channels[\n",
    "                          -1::-self._skip_channel_counts]\n",
    "\n",
    "            # Extract LFP data at specified channels and convert to uV\n",
    "            lfp = intantech_recording.lf_timeseries[:, lfp_channel_ind]  # (sample x channel)\n",
    "            lfp = (lfp * intantech_recording.get_channel_bit_volts('lf')[lfp_channel_ind]).T  # (channel x sample)\n",
    "\n",
    "            self.insert1(dict(key,\n",
    "                              lfp_sampling_rate=intantech_recording.lfmeta.meta['imSampRate'],\n",
    "                              lfp_time_stamps=(np.arange(lfp.shape[1])\n",
    "                                               / intantech_recording.lfmeta.meta['imSampRate']),\n",
    "                              lfp_mean=lfp.mean(axis=0)))\n",
    "\n",
    "            electrode_query = (probe.ProbeType.Electrode\n",
    "                               * probe.ElectrodeConfig.Electrode\n",
    "                               * EphysRecording & key)\n",
    "            probe_electrodes = {\n",
    "                (shank, shank_col, shank_row): key\n",
    "                for key, shank, shank_col, shank_row in zip(*electrode_query.fetch(\n",
    "                    'KEY', 'shank', 'shank_col', 'shank_row'))}\n",
    "\n",
    "            for recorded_site in lfp_channel_ind:\n",
    "                shank, shank_col, shank_row, _ = intantech_recording.apmeta.shankmap['data'][recorded_site]\n",
    "                electrode_keys.append(probe_electrodes[(shank, shank_col, shank_row)])\n",
    "        elif acq_software == 'Open Ephys':\n",
    "            \n",
    "            session_dir = find_full_path(get_ephys_root_data_dir(), \n",
    "                                         get_session_directory(key))\n",
    "\n",
    "            loaded_oe = openephys.OpenEphys(session_dir)\n",
    "            oe_probe = loaded_oe.probes[probe_sn]\n",
    "\n",
    "            lfp_channel_ind = np.arange(\n",
    "                len(oe_probe.lfp_meta['channels_ids']))[-1::-self._skip_channel_counts]\n",
    "\n",
    "            lfp = oe_probe.lfp_timeseries[:, lfp_channel_ind]  # (sample x channel)\n",
    "            lfp = (lfp * np.array(oe_probe.lfp_meta['channels_gains'])[lfp_channel_ind]).T  # (channel x sample)\n",
    "            lfp_timestamps = oe_probe.lfp_timestamps\n",
    "\n",
    "            self.insert1(dict(key,\n",
    "                              lfp_sampling_rate=oe_probe.lfp_meta['sample_rate'],\n",
    "                              lfp_time_stamps=lfp_timestamps,\n",
    "                              lfp_mean=lfp.mean(axis=0)))\n",
    "\n",
    "            electrode_query = (probe.ProbeType.Electrode\n",
    "                               * probe.ElectrodeConfig.Electrode\n",
    "                               * EphysRecording & key)\n",
    "            probe_electrodes = {key['electrode']: key\n",
    "                                for key in electrode_query.fetch('KEY')}\n",
    "\n",
    "            for channel_idx in np.array(oe_probe.lfp_meta['channels_ids'])[lfp_channel_ind]:\n",
    "                electrode_keys.append(probe_electrodes[channel_idx])\n",
    "        else:\n",
    "            raise NotImplementedError(f'LFP extraction from acquisition software'\n",
    "                                      f' of type {acq_software} is not yet implemented')\n",
    "\n",
    "        # single insert in loop to mitigate potential memory issue\n",
    "        for electrode_key, lfp_trace in zip(electrode_keys, lfp):\n",
    "            self.Electrode.insert1({**key, **electrode_key, 'lfp': lfp_trace})\n",
    "\n",
    "\n",
    "# ------------ Clustering --------------\n",
    "\n",
    "@schema\n",
    "class ClusteringMethod(dj.Lookup):\n",
    "    definition = \"\"\"\n",
    "    # Method for clustering\n",
    "    clustering_method: varchar(16)\n",
    "    ---\n",
    "    clustering_method_desc: varchar(1000)\n",
    "    \"\"\"\n",
    "\n",
    "    contents = [('kilosort', 'kilosort clustering method'),\n",
    "                ('kilosort2', 'kilosort2 clustering method')]\n",
    "\n",
    "\n",
    "@schema\n",
    "class ClusteringParamSet(dj.Lookup):\n",
    "    definition = \"\"\"\n",
    "    # Parameter set to be used in a clustering procedure\n",
    "    paramset_idx:  smallint\n",
    "    ---\n",
    "    -> ClusteringMethod    \n",
    "    paramset_desc: varchar(128)\n",
    "    param_set_hash: uuid\n",
    "    unique index (param_set_hash)\n",
    "    params: longblob  # dictionary of all applicable parameters\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def insert_new_params(cls, processing_method: str, paramset_idx: int,\n",
    "                          paramset_desc: str, params: dict):\n",
    "        param_dict = {'clustering_method': processing_method,\n",
    "                      'paramset_idx': paramset_idx,\n",
    "                      'paramset_desc': paramset_desc,\n",
    "                      'params': params,\n",
    "                      'param_set_hash':  dict_to_uuid(params)}\n",
    "        param_query = cls & {'param_set_hash': param_dict['param_set_hash']}\n",
    "\n",
    "        if param_query:  # If the specified param-set already exists\n",
    "            existing_paramset_idx = param_query.fetch1('paramset_idx')\n",
    "            if existing_paramset_idx == paramset_idx:  # If the existing set has the same paramset_idx: job done\n",
    "                return\n",
    "            else:  # If not same name: human error, trying to add the same paramset with different name\n",
    "                raise dj.DataJointError(\n",
    "                    'The specified param-set'\n",
    "                    ' already exists - paramset_idx: {}'.format(existing_paramset_idx))\n",
    "        else:\n",
    "            cls.insert1(param_dict)\n",
    "\n",
    "\n",
    "@schema\n",
    "class ClusterQualityLabel(dj.Lookup):\n",
    "    definition = \"\"\"\n",
    "    # Quality\n",
    "    cluster_quality_label:  varchar(100)\n",
    "    ---\n",
    "    cluster_quality_description:  varchar(4000)\n",
    "    \"\"\"\n",
    "    contents = [\n",
    "        ('good', 'single unit'),\n",
    "        ('ok', 'probably a single unit, but could be contaminated'),\n",
    "        ('mua', 'multi-unit activity'),\n",
    "        ('noise', 'bad unit')\n",
    "    ]\n",
    "\n",
    "\n",
    "@schema\n",
    "class ClusteringTask(dj.Manual):\n",
    "    definition = \"\"\"\n",
    "    # Manual table for defining a clustering task ready to be run\n",
    "    -> EphysRecording\n",
    "    -> ClusteringParamSet\n",
    "    ---\n",
    "    clustering_output_dir: varchar(255)  #  clustering output directory relative to the clustering root data directory\n",
    "    task_mode='load': enum('load', 'trigger')  # 'load': load computed analysis results, 'trigger': trigger computation\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "@schema\n",
    "class Clustering(dj.Imported):\n",
    "    \"\"\"\n",
    "    A processing table to handle each ClusteringTask:\n",
    "    + If `task_mode == \"trigger\"`: trigger clustering analysis\n",
    "        according to the ClusteringParamSet (e.g. launch a kilosort job)\n",
    "    + If `task_mode == \"load\"`: verify output\n",
    "    \"\"\"\n",
    "    definition = \"\"\"\n",
    "    # Clustering Procedure\n",
    "    -> ClusteringTask\n",
    "    ---\n",
    "    clustering_time: datetime  # time of generation of this set of clustering results \n",
    "    package_version='': varchar(16)\n",
    "    \"\"\"\n",
    "\n",
    "    def make(self, key):\n",
    "        task_mode, output_dir = (ClusteringTask & key).fetch1(\n",
    "            'task_mode', 'clustering_output_dir')\n",
    "        kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n",
    "\n",
    "        if task_mode == 'load':\n",
    "            kilosort_dataset = kilosort.Kilosort(kilosort_dir)  # check if the directory is a valid Kilosort output\n",
    "            creation_time, _, _ = kilosort.extract_clustering_info(kilosort_dir)\n",
    "        elif task_mode == 'trigger':\n",
    "            raise NotImplementedError('Automatic triggering of'\n",
    "                                      ' clustering analysis is not yet supported')\n",
    "        else:\n",
    "            raise ValueError(f'Unknown task mode: {task_mode}')\n",
    "\n",
    "        self.insert1({**key, 'clustering_time': creation_time})\n",
    "\n",
    "\n",
    "@schema\n",
    "class Curation(dj.Manual):\n",
    "    definition = \"\"\"\n",
    "    # Manual curation procedure\n",
    "    -> Clustering\n",
    "    curation_id: int\n",
    "    ---\n",
    "    curation_time: datetime             # time of generation of this set of curated clustering results \n",
    "    curation_output_dir: varchar(255)   # output directory of the curated results, relative to root data directory\n",
    "    quality_control: bool               # has this clustering result undergone quality control?\n",
    "    manual_curation: bool               # has manual curation been performed on this clustering result?\n",
    "    curation_note='': varchar(2000)  \n",
    "    \"\"\"\n",
    "\n",
    "    def create1_from_clustering_task(self, key, curation_note=''):\n",
    "        \"\"\"\n",
    "        A function to create a new corresponding \"Curation\" for a particular \n",
    "        \"ClusteringTask\"\n",
    "        \"\"\"\n",
    "        if key not in Clustering():\n",
    "            raise ValueError(f'No corresponding entry in Clustering available'\n",
    "                             f' for: {key}; do `Clustering.populate(key)`')\n",
    "\n",
    "        task_mode, output_dir = (ClusteringTask & key).fetch1(\n",
    "            'task_mode', 'clustering_output_dir')\n",
    "        kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n",
    "\n",
    "        creation_time, is_curated, is_qc = kilosort.extract_clustering_info(kilosort_dir)\n",
    "        # Synthesize curation_id\n",
    "        curation_id = dj.U().aggr(self & key, n='ifnull(max(curation_id)+1,1)').fetch1('n')\n",
    "        self.insert1({**key, 'curation_id': curation_id,\n",
    "                      'curation_time': creation_time, \n",
    "                      'curation_output_dir': output_dir,\n",
    "                      'quality_control': is_qc, \n",
    "                      'manual_curation': is_curated,\n",
    "                      'curation_note': curation_note})\n",
    "\n",
    "\n",
    "@schema\n",
    "class CuratedClustering(dj.Imported):\n",
    "    definition = \"\"\"\n",
    "    # Clustering results of a curation.\n",
    "    -> Curation    \n",
    "    \"\"\"\n",
    "\n",
    "    class Unit(dj.Part):\n",
    "        definition = \"\"\"   \n",
    "        # Properties of a given unit from a round of clustering (and curation)\n",
    "        -> master\n",
    "        unit: int\n",
    "        ---\n",
    "        -> probe.ElectrodeConfig.Electrode  # electrode with highest waveform amplitude for this unit\n",
    "        -> ClusterQualityLabel\n",
    "        spike_count: int         # how many spikes in this recording for this unit\n",
    "        spike_times: longblob    # (s) spike times of this unit, relative to the start of the EphysRecording\n",
    "        spike_sites : longblob   # array of electrode associated with each spike\n",
    "        spike_depths : longblob  # (um) array of depths associated with each spike, relative to the (0, 0) of the probe    \n",
    "        \"\"\"\n",
    "\n",
    "    def make(self, key):\n",
    "        output_dir = (Curation & key).fetch1('curation_output_dir')\n",
    "        kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n",
    "\n",
    "        kilosort_dataset = kilosort.Kilosort(kilosort_dir)\n",
    "        acq_software = (EphysRecording & key).fetch1('acq_software')\n",
    "\n",
    "        # ---------- Unit ----------\n",
    "        # -- Remove 0-spike units\n",
    "        withspike_idx = [i for i, u in enumerate(kilosort_dataset.data['cluster_ids'])\n",
    "                         if (kilosort_dataset.data['spike_clusters'] == u).any()]\n",
    "        valid_units = kilosort_dataset.data['cluster_ids'][withspike_idx]\n",
    "        valid_unit_labels = kilosort_dataset.data['cluster_groups'][withspike_idx]\n",
    "        # -- Get channel and electrode-site mapping\n",
    "        channel2electrodes = get_neuropixels_channel2electrode_map(key, acq_software)\n",
    "\n",
    "        # -- Spike-times --\n",
    "        # spike_times_sec_adj > spike_times_sec > spike_times\n",
    "        spike_time_key = ('spike_times_sec_adj' if 'spike_times_sec_adj' in kilosort_dataset.data\n",
    "                          else 'spike_times_sec' if 'spike_times_sec'\n",
    "                                                    in kilosort_dataset.data else 'spike_times')\n",
    "        spike_times = kilosort_dataset.data[spike_time_key]\n",
    "        kilosort_dataset.extract_spike_depths()\n",
    "\n",
    "        # -- Spike-sites and Spike-depths --\n",
    "        spike_sites = np.array([channel2electrodes[s]['electrode']\n",
    "                                for s in kilosort_dataset.data['spike_sites']])\n",
    "        spike_depths = kilosort_dataset.data['spike_depths']\n",
    "\n",
    "        # -- Insert unit, label, peak-chn\n",
    "        units = []\n",
    "        for unit, unit_lbl in zip(valid_units, valid_unit_labels):\n",
    "            if (kilosort_dataset.data['spike_clusters'] == unit).any():\n",
    "                unit_channel, _ = kilosort_dataset.get_best_channel(unit)\n",
    "                unit_spike_times = (spike_times[kilosort_dataset.data['spike_clusters'] == unit]\n",
    "                                    / kilosort_dataset.data['params']['sample_rate'])\n",
    "                spike_count = len(unit_spike_times)\n",
    "\n",
    "                units.append({\n",
    "                    'unit': unit,\n",
    "                    'cluster_quality_label': unit_lbl,\n",
    "                    **channel2electrodes[unit_channel],\n",
    "                    'spike_times': unit_spike_times,\n",
    "                    'spike_count': spike_count,\n",
    "                    'spike_sites': spike_sites[kilosort_dataset.data['spike_clusters'] == unit],\n",
    "                    'spike_depths': spike_depths[kilosort_dataset.data['spike_clusters'] == unit]})\n",
    "\n",
    "        self.insert1(key)\n",
    "        self.Unit.insert([{**key, **u} for u in units])\n",
    "\n",
    "\n",
    "@schema\n",
    "class WaveformSet(dj.Imported):\n",
    "    definition = \"\"\"\n",
    "    # A set of spike waveforms for units out of a given CuratedClustering\n",
    "    -> CuratedClustering\n",
    "    \"\"\"\n",
    "\n",
    "    class PeakWaveform(dj.Part):\n",
    "        definition = \"\"\"\n",
    "        # Mean waveform across spikes for a given unit at its representative electrode\n",
    "        -> master\n",
    "        -> CuratedClustering.Unit\n",
    "        ---\n",
    "        peak_electrode_waveform: longblob  # (uV) mean waveform for a given unit at its representative electrode\n",
    "        \"\"\"\n",
    "\n",
    "    class Waveform(dj.Part):\n",
    "        definition = \"\"\"\n",
    "        # Spike waveforms and their mean across spikes for the given unit\n",
    "        -> master\n",
    "        -> CuratedClustering.Unit\n",
    "        -> probe.ElectrodeConfig.Electrode  \n",
    "        --- \n",
    "        waveform_mean: longblob   # (uV) mean waveform across spikes of the given unit\n",
    "        waveforms=null: longblob  # (uV) (spike x sample) waveforms of a sampling of spikes at the given electrode for the given unit\n",
    "        \"\"\"\n",
    "\n",
    "    def make(self, key):\n",
    "        output_dir = (Curation & key).fetch1('curation_output_dir')\n",
    "        kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n",
    "\n",
    "        kilosort_dataset = kilosort.Kilosort(kilosort_dir)\n",
    "\n",
    "        acq_software, probe_serial_number = (EphysRecording * ProbeInsertion & key).fetch1(\n",
    "            'acq_software', 'probe')\n",
    "\n",
    "        # -- Get channel and electrode-site mapping\n",
    "        recording_key = (EphysRecording & key).fetch1('KEY')\n",
    "        channel2electrodes = get_neuropixels_channel2electrode_map(recording_key, acq_software)\n",
    "\n",
    "        is_qc = (Curation & key).fetch1('quality_control')\n",
    "\n",
    "        # Get all units\n",
    "        units = {u['unit']: u for u in (CuratedClustering.Unit & key).fetch(\n",
    "            as_dict=True, order_by='unit')}\n",
    "\n",
    "        if is_qc:\n",
    "            unit_waveforms = np.load(kilosort_dir / 'mean_waveforms.npy')  # unit x channel x sample\n",
    "\n",
    "            def yield_unit_waveforms():\n",
    "                for unit_no, unit_waveform in zip(kilosort_dataset.data['cluster_ids'],\n",
    "                                                  unit_waveforms):\n",
    "                    unit_peak_waveform = {}\n",
    "                    unit_electrode_waveforms = []\n",
    "                    if unit_no in units:\n",
    "                        for channel, channel_waveform in zip(\n",
    "                                kilosort_dataset.data['channel_map'],\n",
    "                                unit_waveform):\n",
    "                            unit_electrode_waveforms.append({\n",
    "                                **units[unit_no], **channel2electrodes[channel],\n",
    "                                'waveform_mean': channel_waveform})\n",
    "                            if channel2electrodes[channel]['electrode'] == units[unit_no]['electrode']:\n",
    "                                unit_peak_waveform = {\n",
    "                                    **units[unit_no],\n",
    "                                    'peak_electrode_waveform': channel_waveform}\n",
    "                    yield unit_peak_waveform, unit_electrode_waveforms\n",
    "        else:\n",
    "            if acq_software == 'IntanTECH':\n",
    "                intantech_meta_filepath = get_intantech_meta_filepath(key)\n",
    "                neuropixels_recording = intantech.IntanTECH(intantech_meta_filepath.parent)\n",
    "            elif acq_software == 'Open Ephys':\n",
    "                session_dir = find_full_path(get_ephys_root_data_dir(), \n",
    "                                             get_session_directory(key))\n",
    "                openephys_dataset = openephys.OpenEphys(session_dir)\n",
    "                neuropixels_recording = openephys_dataset.probes[probe_serial_number]\n",
    "\n",
    "            def yield_unit_waveforms():\n",
    "                for unit_dict in units.values():\n",
    "                    unit_peak_waveform = {}\n",
    "                    unit_electrode_waveforms = []\n",
    "\n",
    "                    spikes = unit_dict['spike_times']\n",
    "                    waveforms = neuropixels_recording.extract_spike_waveforms(\n",
    "                        spikes, kilosort_dataset.data['channel_map'])  # (sample x channel x spike)\n",
    "                    waveforms = waveforms.transpose((1, 2, 0))  # (channel x spike x sample)\n",
    "                    for channel, channel_waveform in zip(\n",
    "                            kilosort_dataset.data['channel_map'], waveforms):\n",
    "                        unit_electrode_waveforms.append({\n",
    "                            **unit_dict, **channel2electrodes[channel],\n",
    "                            'waveform_mean': channel_waveform.mean(axis=0),\n",
    "                            'waveforms': channel_waveform})\n",
    "                        if channel2electrodes[channel]['electrode'] == unit_dict['electrode']:\n",
    "                            unit_peak_waveform = {\n",
    "                                **unit_dict,\n",
    "                                'peak_electrode_waveform': channel_waveform.mean(axis=0)}\n",
    "\n",
    "                    yield unit_peak_waveform, unit_electrode_waveforms\n",
    "\n",
    "        # insert waveform on a per-unit basis to mitigate potential memory issue\n",
    "        self.insert1(key)\n",
    "        for unit_peak_waveform, unit_electrode_waveforms in yield_unit_waveforms():\n",
    "            self.PeakWaveform.insert1(unit_peak_waveform, ignore_extra_fields=True)\n",
    "            self.Waveform.insert(unit_electrode_waveforms, ignore_extra_fields=True)\n",
    "\n",
    "\n",
    "# ---------------- HELPER FUNCTIONS ----------------\n",
    "\n",
    "def get_intantech_meta_filepath(ephys_recording_key):\n",
    "    # attempt to retrieve from EphysRecording.EphysFile\n",
    "    intantech_meta_filepath = (EphysRecording.EphysFile & ephys_recording_key\n",
    "                              & 'file_path LIKE \"%.ap.meta\"').fetch1('file_path')\n",
    "\n",
    "    try:\n",
    "        intantech_meta_filepath = find_full_path(get_ephys_root_data_dir(),\n",
    "                                                intantech_meta_filepath)\n",
    "    except FileNotFoundError:\n",
    "        # if not found, search in session_dir again\n",
    "        if not intantech_meta_filepath.exists():\n",
    "            session_dir = find_full_path(get_ephys_root_data_dir(), \n",
    "                                         get_session_directory(\n",
    "                                             ephys_recording_key))\n",
    "            inserted_probe_serial_number = (ProbeInsertion * probe.Probe\n",
    "                                            & ephys_recording_key).fetch1('probe')\n",
    "\n",
    "            intantech_meta_filepaths = [fp for fp in session_dir.rglob('*.ap.meta')]\n",
    "            for meta_filepath in intantech_meta_filepaths:\n",
    "                intantech_meta = intantech.IntanTECH(meta_filepath)\n",
    "                if str(intantech_meta.probe_SN) == inserted_probe_serial_number:\n",
    "                    intantech_meta_filepath = meta_filepath\n",
    "                    break\n",
    "            else:\n",
    "                raise FileNotFoundError(\n",
    "                    'No IntanTECH data found for probe insertion: {}'.format(ephys_recording_key))\n",
    "\n",
    "    return intantech_meta_filepath\n",
    "\n",
    "\n",
    "def get_neuropixels_channel2electrode_map(ephys_recording_key, acq_software):\n",
    "    if acq_software == 'IntanTECH':\n",
    "        intantech_meta_filepath = get_intantech_meta_filepath(ephys_recording_key)\n",
    "        intantech_meta = intantech.IntanTECHMeta(intantech_meta_filepath)\n",
    "        electrode_config_key = (EphysRecording * probe.ElectrodeConfig\n",
    "                                & ephys_recording_key).fetch1('KEY')\n",
    "\n",
    "        electrode_query = (probe.ProbeType.Electrode\n",
    "                           * probe.ElectrodeConfig.Electrode & electrode_config_key)\n",
    "\n",
    "        probe_electrodes = {\n",
    "            (shank, shank_col, shank_row): key\n",
    "            for key, shank, shank_col, shank_row in zip(*electrode_query.fetch(\n",
    "                'KEY', 'shank', 'shank_col', 'shank_row'))}\n",
    "\n",
    "        channel2electrode_map = {\n",
    "            recorded_site: probe_electrodes[(shank, shank_col, shank_row)]\n",
    "            for recorded_site, (shank, shank_col, shank_row, _) in enumerate(\n",
    "                intantech_meta.shankmap['data'])}\n",
    "    elif acq_software == 'Open Ephys':\n",
    "        session_dir = find_full_path(get_ephys_root_data_dir(), \n",
    "                                     get_session_directory(ephys_recording_key))\n",
    "        openephys_dataset = openephys.OpenEphys(session_dir)\n",
    "        probe_serial_number = (ProbeInsertion & ephys_recording_key).fetch1('probe')\n",
    "        probe_dataset = openephys_dataset.probes[probe_serial_number]\n",
    "\n",
    "        electrode_query = (probe.ProbeType.Electrode\n",
    "                           * probe.ElectrodeConfig.Electrode\n",
    "                           * EphysRecording & ephys_recording_key)\n",
    "\n",
    "        probe_electrodes = {key['electrode']: key\n",
    "                            for key in electrode_query.fetch('KEY')}\n",
    "\n",
    "        channel2electrode_map = {\n",
    "            channel_idx: probe_electrodes[channel_idx]\n",
    "            for channel_idx in probe_dataset.ap_meta['channels_ids']}\n",
    "\n",
    "    return channel2electrode_map\n",
    "\n",
    "\n",
    "def generate_electrode_config(probe_type: str, electrodes: list):\n",
    "    \"\"\"\n",
    "    Generate and insert new ElectrodeConfig\n",
    "    :param probe_type: probe type (e.g. neuropixels 2.0 - SS)\n",
    "    :param electrodes: list of the electrode dict (keys of the probe.ProbeType.Electrode table)\n",
    "    :return: a dict representing a key of the probe.ElectrodeConfig table\n",
    "    \"\"\"\n",
    "    # compute hash for the electrode config (hash of dict of all ElectrodeConfig.Electrode)\n",
    "    electrode_config_hash = dict_to_uuid({k['electrode']: k for k in electrodes})\n",
    "\n",
    "    electrode_list = sorted([k['electrode'] for k in electrodes])\n",
    "    electrode_gaps = ([-1]\n",
    "                      + np.where(np.diff(electrode_list) > 1)[0].tolist()\n",
    "                      + [len(electrode_list) - 1])\n",
    "    electrode_config_name = '; '.join([\n",
    "        f'{electrode_list[start + 1]}-{electrode_list[end]}'\n",
    "        for start, end in zip(electrode_gaps[:-1], electrode_gaps[1:])])\n",
    "\n",
    "    electrode_config_key = {'electrode_config_hash': electrode_config_hash}\n",
    "\n",
    "    # ---- make new ElectrodeConfig if needed ----\n",
    "    if not probe.ElectrodeConfig & electrode_config_key:\n",
    "        probe.ElectrodeConfig.insert1({**electrode_config_key, 'probe_type': probe_type,\n",
    "                                       'electrode_config_name': electrode_config_name})\n",
    "        probe.ElectrodeConfig.Electrode.insert({**electrode_config_key, **electrode}\n",
    "                                               for electrode in electrodes)\n",
    "\n",
    "    return electrode_config_key"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
