{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84d3e719",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import re\n",
    "\n",
    "import posixpath as ufs\n",
    "\n",
    "from itertools import chain\n",
    "from itertools import repeat\n",
    "\n",
    "from hashlib import sha256 as sha\n",
    "\n",
    "from minio import Minio\n",
    "from datajoint import config as cfg\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class LoggingContext:\n",
    "    '''\n",
    "    Conditional logging context manager via:\n",
    "    https://docs.python.org/3/howto/logging-cookbook.html\n",
    "    '''\n",
    "    def __init__(self, logger, level=None, handler=None, close=True):\n",
    "        self.logger = logger\n",
    "        self.level = level\n",
    "        self.handler = handler\n",
    "        self.close = close\n",
    "\n",
    "    def __enter__(self):\n",
    "        if self.level is not None:\n",
    "            self.old_level = self.logger.level\n",
    "            self.logger.setLevel(self.level)\n",
    "        if self.handler:\n",
    "            self.logger.addHandler(self.handler)\n",
    "\n",
    "    def __exit__(self, et, ev, tb):\n",
    "        if self.level is not None:\n",
    "            self.logger.setLevel(self.old_level)\n",
    "        if self.handler:\n",
    "            self.logger.removeHandler(self.handler)\n",
    "        if self.handler and self.close:\n",
    "            self.handler.close()\n",
    "        # implicit return of None => don't swallow exceptions\n",
    "\n",
    "\n",
    "class DJArchiveClient(object):\n",
    "    '''\n",
    "    Archive Client class - manages operations to s3/djarchive\n",
    "    '''\n",
    "\n",
    "    FILENAME_FILTER = r'^\\.'\n",
    "    MANIFEST_FNAME = 'djarchive-manifest.csv'\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        '''\n",
    "        Create a DJArchiveClient.\n",
    "        Normal client code should use the 'client' method.\n",
    "        '''\n",
    "        log.debug('kwargs: {}'.format(dict(kwargs, secret_key='*REDACTED*')))\n",
    "\n",
    "        self.bucket = kwargs['bucket']\n",
    "        self.endpoint = kwargs['endpoint']\n",
    "        self.access_key = kwargs['access_key']\n",
    "        self.secret_key = kwargs['secret_key']\n",
    "\n",
    "        self.filename_filter = kwargs.get('filename_filter',\n",
    "                                          DJArchiveClient.FILENAME_FILTER)\n",
    "\n",
    "        self.client = Minio(self.endpoint, access_key=self.access_key,\n",
    "                            secret_key=self.secret_key)\n",
    "\n",
    "    @classmethod\n",
    "    def client(cls, admin=False):\n",
    "        '''\n",
    "        Create a DJArchiveClient.\n",
    "        Currently:\n",
    "            Admin usage expects dj.config['custom'] values for:\n",
    "              - djarchive.access_key\n",
    "              - djarchive.secret_key\n",
    "            Client and admin usage allow overriding dj.config['custom']\n",
    "            defaults for:\n",
    "              - djarchive.bucket\n",
    "              - djarchive.endpoint\n",
    "        The configuration mechanism is expected to change to allow for\n",
    "        more general purpose client usage without requiring extra\n",
    "        configuration.\n",
    "        '''\n",
    "        log.debug('admin: {}'.format(admin))\n",
    "\n",
    "        dj_custom = cfg.get('custom', {})\n",
    "\n",
    "        cfg_defaults = {\n",
    "            'djarchive.bucket': 'djhub.vathes.datapub.elements',\n",
    "            'djarchive.endpoint': 's3.djhub.io',\n",
    "            'djarchive.filename_filter': DJArchiveClient.FILENAME_FILTER\n",
    "        }\n",
    "\n",
    "        create_args = {k: {**cfg_defaults, **dj_custom}.get(\n",
    "            'djarchive.{}'.format(k), None)\n",
    "                       for k in ('endpoint', 'access_key', 'secret_key',\n",
    "                                 'bucket', 'filename_filter')}\n",
    "\n",
    "        if admin and not all(('access_key' in create_args,\n",
    "                              'secret_key' in create_args)):\n",
    "\n",
    "            raise AttributeError('admin operation requested w/o credentials.')\n",
    "\n",
    "        return cls(**create_args)\n",
    "\n",
    "    def _filter_path(self, fp):\n",
    "        '''\n",
    "        filter normalized paths according to self.filename_filter\n",
    "        Returns fp if path passes filter, else None.\n",
    "        '''\n",
    "        # XXX: didn't override os.walk; intermediate path filtering problematic\n",
    "        #  ex: /some/.foo/junk/file:\n",
    "        #\n",
    "        #    - 'dirs' doesn't impact iteration in DFS mode\n",
    "        #    - '/some/.foo/junk' doesn't match as a 'root'\n",
    "\n",
    "        rx = re.compile(self.filename_filter if self.filename_filter else '^$')\n",
    "\n",
    "        if not any([rx.match(i) for i in fp.split(ufs.sep)]):\n",
    "            return fp\n",
    "\n",
    "    def _manifest(self, filepath):\n",
    "        '''\n",
    "        Compute the manifest data for the file at filepath.\n",
    "        Function returns size in bytes and the sha256 hex digest of the file.\n",
    "        Does not perform path normalization to/from posix path as used within\n",
    "        the manifest file.\n",
    "        '''\n",
    "        fp_sz = os.stat(filepath).st_size\n",
    "\n",
    "        fp_sha = sha()\n",
    "\n",
    "        rd_sz = 1024 * 64\n",
    "\n",
    "        with open(filepath, 'rb') as fh:\n",
    "            dat = fh.read(rd_sz)\n",
    "            while dat:\n",
    "                fp_sha.update(dat)\n",
    "                dat = fh.read(rd_sz)\n",
    "\n",
    "        return fp_sz, fp_sha.hexdigest()\n",
    "\n",
    "    def _normalize_path(self, root_directory, filepath):\n",
    "        '''\n",
    "        normlize path from host-local format into storage-side format\n",
    "        (s3/posixpath)\n",
    "        '''\n",
    "\n",
    "        subp = filepath.replace(\n",
    "            os.path.commonprefix(\n",
    "                (root_directory, filepath)), '').lstrip(os.path.sep)\n",
    "\n",
    "        return subp.replace(os.path.sep, ufs.sep)\n",
    "\n",
    "    def _denormalize_path(self, root_directory, subpath):\n",
    "        '''\n",
    "        denormlize path from storage-side format into host-local format\n",
    "        (os.path)\n",
    "        '''\n",
    "\n",
    "        subpath = subpath.replace(ufs.sep, os.path.sep)\n",
    "\n",
    "        return os.path.join(root_directory, subpath)\n",
    "\n",
    "    def write_manifest(self, source_directory, overwrite=False):\n",
    "        '''\n",
    "        create a manifest for source_directory.\n",
    "        manifest is of the form:\n",
    "          size(bytes),hex(sha256),posixpath(subpath)\n",
    "          ...\n",
    "        '''\n",
    "        log.debug('source_directory: {}, overwrite: {}'.format(\n",
    "            source_directory, overwrite))\n",
    "\n",
    "        # XXX: some logic duplicated in _upload_creating_manifest -\n",
    "        #      adjustments here should be audited for impact there as well.\n",
    "\n",
    "        mani = os.path.join(source_directory, self.MANIFEST_FNAME)\n",
    "\n",
    "        if os.path.exists(mani) and not overwrite:\n",
    "            msg = 'djarchive manifest {} already exists and overwrite=False'\n",
    "            log.warning(msg)\n",
    "            raise FileExistsError(msg)\n",
    "\n",
    "        with open(mani, 'wb') as mani_fh:\n",
    "\n",
    "            for root, dirs, files in os.walk(source_directory):\n",
    "\n",
    "                for fp in (os.path.join(root, f) for f in files):\n",
    "\n",
    "                    if fp == mani:\n",
    "                        continue\n",
    "\n",
    "                    subp = self._normalize_path(source_directory, fp)\n",
    "\n",
    "                    if not self._filter_path(subp):\n",
    "                        continue\n",
    "\n",
    "                    print(\"adding {}\".format(subp))\n",
    "\n",
    "                    fp_sz, fp_sha = self._manifest(fp)\n",
    "\n",
    "                    ent = '\"{}\",\"{}\",\"{}\"\\n'.format(fp_sz, fp_sha, subp)\n",
    "\n",
    "                    mani_fh.write(ent.encode())\n",
    "\n",
    "    def read_manifest(self, source_directory):\n",
    "        '''\n",
    "        Read the manifest contents for the dataset within source_directory,\n",
    "        if available.\n",
    "        Returns a file-subpath keyed dictionary with each item\n",
    "        containing a dictionary of the given files size & sha.\n",
    "        for example:\n",
    "          {'/etc/passwd': {'size': 512, 'sha': 'deadbeef...'}}\n",
    "        If no manifest exists, a FileNotFoundError is raised.\n",
    "        '''\n",
    "        log.debug('source_directory: {}'.format(source_directory))\n",
    "\n",
    "        mani = os.path.join(source_directory, self.MANIFEST_FNAME)\n",
    "\n",
    "        ret = {}\n",
    "\n",
    "        with open(mani, 'rb') as mani_fh:\n",
    "            for ent in mani_fh:\n",
    "                ent = ent.decode().strip().split(',')\n",
    "                sz, sha, subp = (i.replace('\"', '') for i in ent)\n",
    "\n",
    "                assert subp not in ret  # detect invalid duplicates\n",
    "\n",
    "                ret[subp] = {'size': int(sz), 'sha': sha}\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def upload(self, name, revision, source_directory, display_progress=False):\n",
    "        '''\n",
    "        upload contents of source_directory as the dataset of name/revision\n",
    "        (currently placeholder for API design)\n",
    "        '''\n",
    "\n",
    "        log.debug('name: {}, revision: {}, source_directory: {}'.format(\n",
    "            name, revision, source_directory))\n",
    "\n",
    "        with LoggingContext(log, level=logging.DEBUG if display_progress\n",
    "                            else logging.INFO):\n",
    "\n",
    "            mani_fp = os.path.join(source_directory, self.MANIFEST_FNAME)\n",
    "\n",
    "            if os.path.exists(mani_fp):\n",
    "                self._upload_using_manifest(\n",
    "                    name, revision, source_directory, display_progress)\n",
    "            else:\n",
    "                self._upload_creating_manifest(\n",
    "                    name, revision, source_directory, display_progress)\n",
    "\n",
    "    def _upload_using_manifest(self, name, revision, source_directory,\n",
    "                               display_progress):\n",
    "        '''\n",
    "        Upload dataset which already has a manifest -\n",
    "        Expects source directory to match manifest contents;\n",
    "        Raises FileNotFoundError if files are found in\n",
    "        source_directory not in the manifest, and ValueError if files\n",
    "        are found with manifest size/checksum mismatch.\n",
    "        '''\n",
    "        log.debug('name: {}, revision: {}, source_directory: {}'.format(\n",
    "            name, revision, source_directory))\n",
    "\n",
    "        mani_fp = os.path.join(source_directory, self.MANIFEST_FNAME)\n",
    "\n",
    "        mani_dat = self.read_manifest(source_directory)\n",
    "\n",
    "        for root, dirs, files in os.walk(source_directory):\n",
    "\n",
    "            for fp in (os.path.join(root, f) for f in files):\n",
    "\n",
    "                if fp == mani_fp:  # defer manifest upload until end\n",
    "                    continue\n",
    "\n",
    "                subp = self._normalize_path(source_directory, fp)\n",
    "\n",
    "                # XXX: assumes manifest generation occurs under same filter\n",
    "                if not self._filter_path(subp):\n",
    "                    continue\n",
    "\n",
    "                if subp not in mani_dat:\n",
    "                    msg = 'subpath {} not in manifest'.format(subp)\n",
    "                    log.error(msg)\n",
    "                    raise FileNotFoundError(msg)\n",
    "\n",
    "                fp_sz, fp_sha = self._manifest(fp)\n",
    "\n",
    "                ref_sz, ref_sha = mani_dat[subp]['size'], mani_dat[subp]['sha']\n",
    "\n",
    "                if fp_sz != ref_sz or fp_sha != ref_sha:\n",
    "\n",
    "                    msg = 'manifest mismatch for {}'.format(subp)\n",
    "                    msg += ' (sz: {} / ref: {})'.format(fp_sz, ref_sz)\n",
    "                    msg += ' (sha: {} / ref: {})'.format(fp_sha, ref_sha)\n",
    "\n",
    "                    log.error(msg)\n",
    "\n",
    "                    raise ValueError(msg)\n",
    "\n",
    "                dstp = ufs.join(name, revision, subp)\n",
    "\n",
    "                self.fput_object(dstp, fp)\n",
    "\n",
    "        # upload of files complete - send manifest to indicate completeness.\n",
    "        self.fput_object(ufs.join(name, revision, self.MANIFEST_FNAME),\n",
    "                         mani_fp, display_progress)\n",
    "\n",
    "    def _upload_creating_manifest(self, name, revision, source_directory,\n",
    "                                  display_progress):\n",
    "        '''\n",
    "        Upload dataset without manifest -\n",
    "        Manifest will be generated as part of the upload process.\n",
    "        '''\n",
    "        log.debug('name: {}, revision: {}, source_directory: {}'.format(\n",
    "            name, revision, source_directory))\n",
    "\n",
    "        # XXX: some logic duplicated from write_manifest -\n",
    "        #      adjustments here should be audited for impact there as well.\n",
    "\n",
    "        mani_fp = os.path.join(source_directory, self.MANIFEST_FNAME)\n",
    "\n",
    "        assert not os.path.exists(mani_fp)\n",
    "\n",
    "        with open(mani_fp, 'wb') as mani_fh:\n",
    "\n",
    "            for root, dirs, files in os.walk(source_directory):\n",
    "\n",
    "                for fp in (os.path.join(root, f) for f in files):\n",
    "\n",
    "                    if fp == mani_fp:  # defer manifest upload until end\n",
    "                        continue\n",
    "\n",
    "                    subp = self._normalize_path(source_directory, fp)\n",
    "\n",
    "                    if not self._filter_path(subp):\n",
    "                        continue\n",
    "\n",
    "                    fp_sz, fp_sha = self._manifest(fp)\n",
    "\n",
    "                    dstp = ufs.join(name, revision, subp)\n",
    "\n",
    "                    self.fput_object(dstp, fp)\n",
    "\n",
    "                    ent = '\"{}\",\"{}\",\"{}\"\\n'.format(fp_sz, fp_sha, subp)\n",
    "\n",
    "                    mani_fh.write(ent.encode())\n",
    "\n",
    "        # upload of files complete - send manifest to indicate completeness.\n",
    "        self.fput_object(ufs.join(name, revision, self.MANIFEST_FNAME),\n",
    "                         mani_fp, display_progress)\n",
    "\n",
    "    def redact(name, revision):\n",
    "        '''\n",
    "        redact (revoke) dataset publication of name/revision\n",
    "        (currently placeholder for API design)\n",
    "        XXX: workflow data safety concerns?\n",
    "        '''\n",
    "        raise NotImplementedError('redaction not implemented')\n",
    "\n",
    "    def datasets(self):\n",
    "        '''\n",
    "        return the available datasets as a generator of dataset names\n",
    "        '''\n",
    "\n",
    "        # s3://bucket/dataset -> generator(('dataset'))\n",
    "\n",
    "        for ds in (o for o in self.client.list_objects(self.bucket)\n",
    "                   if o.is_dir):\n",
    "            yield ds.object_name.rstrip('/')\n",
    "\n",
    "    def revisions(self, dataset=None):\n",
    "        '''\n",
    "        return the list of available dataset revisions as a generator\n",
    "        of (dataset_name, dataset_revision) tuples.\n",
    "        when datasest is provided, only return the revisions for that\n",
    "        dataset.\n",
    "        '''\n",
    "        def _revisions(dataset):\n",
    "\n",
    "            # s3://bucket/dataset/revision ->\n",
    "            #    generator(('dataset', 'revision'), ...)\n",
    "\n",
    "            pfx = '{}/'.format(dataset)\n",
    "\n",
    "            for ds in (o for o in self.client.list_objects(\n",
    "                    self.bucket, prefix=pfx) if o.is_dir):\n",
    "\n",
    "                yield tuple(ds.object_name.rstrip('/').split(ufs.sep))\n",
    "\n",
    "        found = False\n",
    "\n",
    "        datasets = (dataset,) if dataset else self.datasets()\n",
    "\n",
    "        for ds in datasets:\n",
    "            for r in _revisions(ds):\n",
    "                found = True\n",
    "                yield r\n",
    "\n",
    "        if dataset and not found:\n",
    "\n",
    "            msg = 'dataset {} not found'.format(dataset)\n",
    "            log.debug(msg)\n",
    "            raise FileNotFoundError(msg)\n",
    "\n",
    "    def download(self, dataset_name, revision, target_directory,\n",
    "                 create_target=False, display_progress=False):\n",
    "        '''\n",
    "        download a dataset's contents into the top-level of target_directory.\n",
    "        when create_target is specified, target_directory and parents\n",
    "        will be created, otherwise, an error is signaled.\n",
    "        Note: display_progress currently means 'enable debug level logging'\n",
    "              within function scope - if details are not displayed,\n",
    "              ensure loggingConfig settings are correct in client code.\n",
    "              (verified OK for default djarchive CLI script)\n",
    "        '''\n",
    "\n",
    "        log.debug(('dataset_name: {}, revision: {}, target_directory: {},'\n",
    "                   'create_target: {}, display_progress: {}').format(\n",
    "                       dataset_name, revision, target_directory,\n",
    "                       create_target, display_progress))\n",
    "\n",
    "        with LoggingContext(log, level=logging.DEBUG if display_progress\n",
    "                            else logging.INFO):\n",
    "\n",
    "            if create_target:  # ensure target directory exists\n",
    "                os.makedirs(target_directory, exist_ok=True)\n",
    "\n",
    "            if not os.path.exists(target_directory):\n",
    "                msg = 'target_directory {} does not exist'.format(\n",
    "                    target_directory)\n",
    "                log.warning(msg)\n",
    "                raise FileNotFoundError(msg)\n",
    "\n",
    "            pfx = ufs.join(dataset_name, revision)\n",
    "\n",
    "            # check/fetch dataset manifest\n",
    "            log.debug('fetching & loading dataset manifest')\n",
    "\n",
    "            ssubp = ufs.join(pfx, self.MANIFEST_FNAME)\n",
    "            lpath = os.path.join(target_directory, self.MANIFEST_FNAME)\n",
    "            lsubd, _ = os.path.split(lpath)\n",
    "\n",
    "            if not self.client.stat_object(self.bucket, ssubp):\n",
    "                msg = 'dataset {} revision {} manifest not found'.format(\n",
    "                    dataset_name, revision)\n",
    "                log.debug(msg)\n",
    "                raise FileNotFoundError(msg)\n",
    "\n",
    "            self.fget_object(ssubp, lpath, display_progress=display_progress)\n",
    "\n",
    "            mani = self.read_manifest(lsubd)\n",
    "\n",
    "            # main download loop -\n",
    "            #\n",
    "            # iterate over objects,\n",
    "            # convert full source path to source subpath,\n",
    "            # construct local path and create local subdirectory in the target\n",
    "            # then fetch the object into the local path.\n",
    "            #\n",
    "            # local paths are dealt with using OS path for native support,\n",
    "            # paths in the s3 space use posixpath since these are '/' delimited\n",
    "\n",
    "            nfound, nerr, oerr = 0, 0, 0  # n files, global & per-object errors\n",
    "\n",
    "            obj_iter = self.client.list_objects(\n",
    "                self.bucket, recursive=True, prefix=pfx)\n",
    "\n",
    "            for obj in obj_iter:\n",
    "\n",
    "                oerr = 0\n",
    "\n",
    "                assert not obj.is_dir  # dirs not in recursive=True output\n",
    "\n",
    "                spath = obj.object_name  # ds/rev/<...?>/thing\n",
    "\n",
    "                ssubp = spath.replace(  # <...?>/thing\n",
    "                    ufs.commonprefix((pfx, spath)), '').lstrip('/')\n",
    "\n",
    "                if ssubp == self.MANIFEST_FNAME:\n",
    "                    continue  # skip manifest re-download\n",
    "\n",
    "                # target_directory/<...?>/thing\n",
    "                lpath = os.path.join(target_directory, *ssubp.split(ufs.sep))\n",
    "                lsubd, _ = os.path.split(lpath)\n",
    "\n",
    "                # ensure we are not creating outside of target_directory\n",
    "                assert (os.path.commonprefix((target_directory, lpath))\n",
    "                        == target_directory)\n",
    "\n",
    "                # actual download/verification -\n",
    "                #\n",
    "                # if the file exists, checksum and skip, falling back to\n",
    "                #    refetch on checksum/size mismatch.\n",
    "                #\n",
    "                # if the file does not exist, download and checksum.\n",
    "\n",
    "                if os.path.exists(lpath):\n",
    "\n",
    "                    log.debug('{} exists. verifying integrity.'.format(lpath))\n",
    "                    lsz, lsha = self._manifest(lpath)\n",
    "\n",
    "                    if all((lsz == mani[ssubp]['size'],\n",
    "                            lsha == mani[ssubp]['sha'])):\n",
    "\n",
    "                        log.debug('integrity check ok. skipping download.')\n",
    "\n",
    "                        nfound += 1  # mark as complete\n",
    "                        continue  # goto for obj in obj_iter\n",
    "\n",
    "                    oerr += 1\n",
    "                    log.warning('integrity issue. redownloading {}'.format(\n",
    "                        spath))\n",
    "\n",
    "                # transfer file\n",
    "                log.debug('transferring {} to {}'.format(spath, lpath))\n",
    "\n",
    "                os.makedirs(lsubd, exist_ok=True)\n",
    "\n",
    "                self.fget_object(spath, lpath, display_progress)\n",
    "\n",
    "                # check file integrity\n",
    "                log.debug('verifying integrity of {}'.format(lpath))\n",
    "\n",
    "                lsz, lsha = self._manifest(lpath)\n",
    "\n",
    "                if lsz != mani[ssubp]['size'] or lsha != mani[ssubp]['sha']:\n",
    "\n",
    "                    oerr += 1\n",
    "\n",
    "                    log.warning('integrity issue fetching {}'.format(spath))\n",
    "\n",
    "                # and mark as complete\n",
    "                nerr += 1 if oerr else 0\n",
    "                nfound += 1\n",
    "\n",
    "            log.info('transfer complete with {} issues.'.format(nerr))\n",
    "\n",
    "            return nfound, nerr\n",
    "\n",
    "    def fget_object(self, spath, lpath, display_progress=False):\n",
    "        '''\n",
    "        Fetch object in spath into local path lpath.\n",
    "        If display_progress=True, a download progress meter will be displayed.\n",
    "        '''\n",
    "        statb = self.client.stat_object(self.bucket, spath)\n",
    "\n",
    "        chunksz = 1024 ** 2  # 1 MiB (TODO? configurable/tuning?)\n",
    "\n",
    "        nchunks, leftover = statb.size // chunksz, statb.size % chunksz\n",
    "\n",
    "        chunker = (chain(repeat(chunksz, nchunks), (leftover,)) if leftover\n",
    "                   else repeat(chunksz, nchunks))\n",
    "\n",
    "        chunker = tqdm(chunker, unit='MiB', ncols=60,\n",
    "                       disable=not display_progress,\n",
    "                       total=nchunks + 1 if leftover else nchunks)\n",
    "\n",
    "        offset = 0\n",
    "        with open(lpath, 'wb') as fh:\n",
    "            for chunk in chunker:\n",
    "                dat = self.client.get_object(\n",
    "                    self.bucket, spath, offset=offset, length=chunk)\n",
    "                fh.write(dat.data)\n",
    "                offset += chunk\n",
    "\n",
    "    def fput_object(self, dpath, lpath, display_progress=False):\n",
    "        '''\n",
    "        Upload file to remote path dpath from lpath.\n",
    "        '''\n",
    "        # TODO: progressbar\n",
    "        # (minio api is inconsistent here - allows a 'progress thread'\n",
    "        #  for whole-file u/l but no per-chunk u/l vs\n",
    "        #  chunked dl and no- 'progress thread' d/l)\n",
    "\n",
    "        log.debug('dpath: {}, lpath: {}'.format(lpath, dpath))\n",
    "        self.client.fput_object(self.bucket, dpath, lpath)\n",
    "\n",
    "\n",
    "client = DJArchiveClient.client  # export factory method as utility function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
